---
title: "LLaMA: Open and Efficient Foundation Language Models"
categories: tech
tags: [Large Language Models]
use_math: true
---

LLaMA (Large Language Model Meta AI) is a series of foundational language models developed by Meta AI. The LLaMA models are designed to be efficient and effective for a wide range of natural language processing tasks. The LLaMA family includes models with various parameter sizes, allowing researchers and developers to choose the model that best fits their needs in terms of performance and computational resources.

Let's take a look at the architecture of LLaMA and how it builds upon the original GPT architecture.

# GPT-style Decoder-Only Transformer: A Technical Overview

## 1. Architecture Overview

The GPT architecture is a **stacked decoder-only Transformer** trained for **causal language modeling**. Unlike the original Transformer which includes both encoder and decoder, GPT uses only the decoder stack, modified with **causal (autoregressive) masking** to prevent information flow from future tokens.

- **Input**: A sequence of token indices $x = [x_1, \dots, x_n]$
- **Output**: A probability distribution over the vocabulary for the next token $x_{t+1}$
- **Objective**: Maximize the log-likelihood

$$
\mathcal{L} = \sum_{t=1}^{n} \log P(x_t \mid x_{<t})
$$

## 2. Model Components

### 2.1 Token Embedding

Each input token $x_t$ is embedded into a $d_{\text{model}}$-dimensional vector using a learned embedding matrix $E \in \mathbb{R}^{V \times d_{\text{model}}}$:

$$
\mathbf{X} \in \mathbb{R}^{n \times d_{\text{model}}}
$$

### 2.2 Positional Encoding

GPT uses **learned absolute positional embeddings**:

$$
\mathbf{P} \in \mathbb{R}^{n \times d_{\text{model}}}
$$

Final input to the first layer:

$$
\mathbf{H}_0 = \mathbf{X} + \mathbf{P}
$$

> LLaMA replaces this with **rotary positional encoding (RoPE)**.

### 2.3 Transformer Block (per layer $\ell$)

Each layer contains:

- Multi-head **causal** self-attention
- Feed-forward network (FFN)
- **Residual connections** and **LayerNorm (post-norm)**

#### a) Causal Self-Attention

For each head:

$$
Q = H_\ell W^Q, \quad K = H_\ell W^K, \quad V = H_\ell W^V
$$

Apply scaled dot-product attention with **causal mask** $M$:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} + M \right)V
$$

Concatenate heads and apply output projection:

$$
\text{MultiHead}(H_\ell) = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W^O
$$

#### b) Feed-Forward Network (FFN)

Standard FFN uses GELU activation:

$$
\text{FFN}(x) = W_2 \cdot \text{GELU}(W_1 x)
$$

#### c) Residual & LayerNorm (Post-Norm)

GPT uses **post-normalization**:

```text
x = x + \text{Sublayer}(\text{LayerNorm}(x))
```

### 2.4 Output Layer
Final hidden state $H_L \in \mathbb{R}^{n \times d_{\text{model}}}$ is projected to logits over the vocabulary:

$$
\text{logits} = H_L \cdot E^\top
$$

GPT uses weight tying: same $E$ is used for input embedding and output projection.

# LLaMA: Open and Efficient Foundation Language Models

Now we move to LLaMA, which is based on the GPT architecture but with some modifications and optimizations.

LLaMA is a **decoder-only Transformer** model, similar to the original GPT architecture. It is designed for autoregressive language modeling, meaning it predicts the next token in a sequence given the previous tokens. The architecture consists of multiple layers of Transformer blocks, each containing self-attention and feed-forward networks.

## 1. Overview

LLaMA (Large Language Model Meta AI) is a family of decoder-only autoregressive Transformers designed to be compute-efficient, open, and competitive with much larger models like GPT-3 and PaLM.

- Architecture: GPT-style, decoder-only Transformer
- Objective: Causal language modeling
- Sizes: 7B, 13B, 33B, 65B
- Training tokens: 1T (7B/13B), 1.4T (33B/65B)
- Data: Only public datasets (CommonCrawl, Wikipedia, ArXiv, GitHub, etc.)
- Innovations: RoPE, SwiGLU, RMSNorm, Chinchilla-inspired scaling

## 2. Core Architecture

LLaMA uses a stack of decoder-only Transformer blocks with architectural improvements. Each Transformer block consists of:

- Pre-normalized input
- Rotary Positional Embedding (RoPE)
- Causal Self-Attention
- SwiGLU Feedforward Network
- Residual connections
- RMSNorm

### 2.1 Token and Positional Embeddings

- Input tokens are mapped to vectors via a learned embedding matrix $E \in \mathbb{R}^{V \times d_{\text{model}}}$.
- LLaMA uses rotary positional embedding (RoPE), applied to the query and key matrices during attention.

There is no explicit positional embedding vector added to the input — RoPE is integrated in the attention mechanism.


### 2.2 Transformer Block (Layer $\ell$)

Each of the $L$ layers is defined as:

$$
x \leftarrow x + \text{Attention}(\text{RMSNorm}(x)) \\
x \leftarrow x + \text{FFN}(\text{RMSNorm}(x))
$$


#### RMSNorm

RMSNorm normalizes via the root mean square without subtracting the mean:

$$
\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{d} \sum_{i=1}^{d} x_i^2 + \epsilon}} \cdot \gamma
$$

#### Self-Attention with RoPE

Causal self-attention is computed as:

$$
\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{\text{RoPE}(Q) \cdot \text{RoPE}(K)^\top}{\sqrt{d_k}} + M \right) V
$$

- $Q$, $K$, $V$ are projections of the normalized input.
- RoPE encodes relative positional information via rotation in complex space.
- $M$ is the causal mask to prevent attention to future tokens.

#### SwiGLU Feedforward Network

The FFN in LLaMA uses SwiGLU activation:

$$
\text{SwiGLU}(x_1, x_2) = x_1 \cdot \text{Swish}(x_2), \quad \text{Swish}(x) = x \cdot \sigma(x)
$$

The FFN computation is:

$$
\text{FFN}(x) = W_3 \cdot \text{SwiGLU}(W_1 x, W_2 x)
$$

Where:
- $W_1$, $W_2$ expand input to intermediate dimension.
- $W_3$ projects back to $d_{\text{model}}$.

### 2.3 Output Projection

After the final layer, the output is projected to vocabulary logits:

$$
\text{logits} = H_L \cdot E^\top
$$

Where:
- $H_L \in \mathbb{R}^{n \times d_{\text{model}}}$ is the final hidden state.
- $E$ is the shared token embedding matrix (weight tying).

## 3. Model Sizes

| Model     | Layers ($L$) | Hidden dim ($d_{\text{model}}$) | Heads | FFN dim ($d_{\text{ffn}}$) | Parameters |
|-----------|--------------|-------------------------------|--------|----------------------------|------------|
| LLaMA-7B  | 32           | 4096                          | 32     | 11008                      | 7B         |
| LLaMA-13B | 40           | 5120                          | 40     | 13824                      | 13B        |
| LLaMA-33B | 60           | 6656                          | 52     | 17920                      | 33B        |
| LLaMA-65B | 80           | 8192                          | 64     | 22016                      | 65B        |

The FFN dimension uses approximately:

$$
d_{\text{ffn}} \approx \frac{2}{3} \cdot 4d_{\text{model}}
$$

## 4. Training Setup

- Optimizer: AdamW
  - $\beta_1 = 0.9$, $\beta_2 = 0.95$
  - Weight decay = 0.1
- Learning rate schedule: cosine decay with warmup
- Sequence length: 2048 tokens
- Precision: bfloat16
- Memory and compute optimization:
  - Activation checkpointing
  - FlashAttention or xformers-style attention kernels
  - Tensor parallelism and sequence parallelism

## 5. Summary Formula

$$
\text{LLaMA} = \text{Decoder-only Transformer} + \text{RoPE} + \text{SwiGLU} + \text{RMSNorm} + \text{Chinchilla Scaling}
$$

## 6. References

- Touvron et al., 2023 — *LLaMA: Open and Efficient Foundation Language Models*
- Brown et al., 2020 — *Language Models are Few-Shot Learners*
- Hoffmann et al., 2022 — *Chinchilla Scaling Laws*
- Su et al., 2021 — *RoFormer: Rotary Position Embedding*
- Shazeer, 2020 — *GLU Variants including SwiGLU*


