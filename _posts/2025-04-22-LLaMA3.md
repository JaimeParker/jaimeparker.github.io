---
title: "The Llama 3 Herd of Models"
categories: tech
tags: [Large Language Models]
use_math: true
---

LLaMA3 技术报告的简要分析。

对于LLMer，最突出的可能是LLaMA3的长上下文支持。在 8,192 个token的序列上训练模型，且通过掩码操作以确保自注意力不会跨越文档边界；同时Llama 3 使用具有 128K tokens的tokenizer；上下文长度也拓展到128K，这对于需要分析大量信息或处理复杂任务的应用程序非常有用。当然还有其处理多模态信息的能力。

但是对于RLer，我们更关心为什么要用DPO而不是PPO。尽管这两种方法在后来的DeepSeek论文中都被证明不如GRPO更简单高效，当然这仅是对于RL finetune LLM而言。至少现在在robotics上PPO仍然是online RL的主流方法。

## References

* [LLaMA3](http://arxiv.org/abs/2407.21783)
* [一文速览Llama 3：从Llama 3的模型架构到如何把长度扩展到100万——基于NTK-aware插值](https://blog.csdn.net/v_JULY_v/article/details/137955982)
* [一文速览Llama 3.1——对其92页paper的全面细致解读：涵盖语言、视觉、语音的架构、原理](https://blog.csdn.net/v_JULY_v/article/details/140659420)
* [提升大模型数据质量的三大要素：找到早期paper且基于GPT摘要出来7方面review(七月论文审稿GPT第4.5/4.6/4.8版)](https://blog.csdn.net/v_JULY_v/article/details/137671187)
* [Introducing Meta Llama 3: The most capable openly available LLM to date](https://ai.meta.com/blog/meta-llama-3/?continueFlag=144175d1f8bc8f3eb7ecba6eaebe2444)

本文将大幅度借鉴（写得太好了，以至于我只想改改格式）[一文速览Llama 3：从Llama 3的模型架构到如何把长度扩展到100万——基于NTK-aware插值](https://blog.csdn.net/v_JULY_v/article/details/137955982)，感谢作者的辛勤付出。

## 1. Introduction

<img src="https://github.com/JaimeParker/jaimeparker.github.io/blob/master/assets/images/llama3_arch.png?raw=true" 
     alt="LLaMA3 Architecture" 
     style="width: 100%; max-width: 600px; display: block; margin: 1em auto;" />
<p style="text-align: center;"><em>Figure: Illustration of the overall architecture and training of Llama 3. Llama 3 is a Transformer language model trained to predict the next token of a textual sequence.</em></p>

LLaMA3的 model architecture 仍然是基于Transformer的语言模型，使用了自注意力机制和前馈神经网络。它的训练目标是预测文本序列的下一个标记。

Comparison between LLaMA3 and LLaMA2:

$$
\text{LLaMA 3} = \text{LLaMA 2} \\
+ \; \textbf{Massive Scaling (405B, 15.6T tokens)} \\
+ \; \textbf{Improved Pretraining Corpus (15T, multilingual, code, reasoning)} \\
+ \; \textbf{Long-Context Support (8K → 128K tokens)} \\
+ \; \textbf{New Tokenizer (128K vocab, better compression)} \\
+ \; \textbf{Improved RoPE Base (θ = 500{,}000)} \\
+ \; \textbf{Post-training with DPO (instead of PPO)} \\
+ \; \textbf{Tool Use, Safety Enhancements, Chat Format} \\
+ \; \textbf{Multimodal Extensions (image, video, speech)} \; [\text{in progress}]
$$

Comparison between LLaMA3 and Transformer:

<img src="https://i-blog.csdnimg.cn/direct/ce9edfe6f5d94524803e6588115be733.png" 
     alt="Transformer VS LLaMA3" 
     style="width: 100%; max-width: 600px; display: block; margin: 1em auto;" />
<p style="text-align: center;"><em>Figure: Transformer VS LLaMA3</em></p>

## 2. General Overview

### 2.1 Language model pre-training

> In the language model pre-training stage, the model learns the structure of language and obtains large amounts of knowledge about the world from the text it is “reading”. To do this effectively, pre-training is performed at massive scale: we pre-train a model with 405B parameters on 15.6T tokens using a context window of 8K tokens. This standard pre-training stage is followed by a continued pre-training stage that increases the supported context window to 128K tokens.

预训练的模型是405B参数，使用了15.6T tokens；增大了处理的tokens，从8K增加到128K tokens。

### 2.2 Language model post-training

> We align the model with human feedback in several rounds, each of which involves supervised finetuning (SFT) on instruction tuning data and Direct Preference Optimization (DPO; Rafailov et al., 2024). At this post-training2 stage, we also integrate new capabilities, such as tool-use, and observe strong improvements in other areas, such as coding and reasoning. See Section 4 for details. Finally, safety mitigations are also incorporated into the model at the post-training stage.
使用更稳定的DPO，在SFT和DPO的多轮迭代中，引入人类反馈。

### 2.3 Multi-modal encoder pre-training

主要介绍image encoder和speech encoder。用 image-text pairs 训练 image encoder。

### 2.4 Vision adapter training

将预训练的 image encoder 集成到预训练的 language model 中。适配器由一系列交叉注意力层(cross-attention)组成，这些层将 image encoder 表示输入到语言模型中。

### 2.5 Speech adapter training

> Finally, we integrate the speech encoder into the model via an adapter that converts speech encodings into token representations that can be fed directly into the finetuned language model.

同样使用一个 adapter 将 speech encoder 的表示转换为 token 表示，之后直接输入到微调的语言模型中。

## 3. Pre-training Improvements in LLaMA 3

### 3.1 Massive Scaling (405B parameters, 15.6T tokens)
- Largest model: 405B parameters
- Pre-trained on 15.6 trillion tokens
- Trained with 3.8×10²⁵ FLOPs
- Uses compute-optimal scaling laws to balance size and data

### 3.2 Improved Pretraining Corpus
- Increased from ~1.8T (LLaMA 2) → 15.6T tokens
- Multilingual, code, reasoning data
- Careful quality filtering, deduplication, and upsampling for math/code

### 3.3 Long-context Training (8K → 128K tokens)
- Gradual expansion during pretraining (6 stages)
- Used RoPE with base θ=500,000 for extended attention range
- Final phase used 800B tokens just for long-context adaptation

### 3.4 New Tokenizer (128K Vocab)
- 128K vocabulary via BPE, better compression than LLaMA 2
- +28K tokens focused on non-English languages
- Compression improved from 3.17 → 3.94 characters/token (English)

## 4. Post-training and Instruction Alignment

指令和chat微调：先奖励建模，然后SFT，最后DPO。

在后期训练中，通过在预训练模型的基础上进行几轮对齐来生成最终的聊天模型。每轮都涉及监督微调 (SFT)、拒绝抽样 (RS) 和直接偏好优化DPO。

* 在预训练基础上使用人类标注数据训练 reward model
* 通过supervised fine-tuning (SFT) 对pre-trained model checkpoint 进行微调；并进一步通过 DPO 对 模型进行对齐。

<img src="https://github.com/JaimeParker/jaimeparker.github.io/blob/master/assets/images/llama3_post_training.png?raw=true" 
     alt="LLaMA3 Post-training" 
     style="width: 100%; max-width: 600px; display: block; margin: 1em auto;" />
<p style="text-align: center;"><em>Figure: Illustration of the overall post-training approach for Llama 3. Our post-training strategy involves rejection sampling, supervised finetuning, and direct preference optimization.</em></p>


### 4.1 SFT (Supervised Fine-Tuning)
- SFT used human-labeled and synthetic data
- High-quality curated dialog data with format alignment (role formatting, system prompts)
- Domain-specific mixes: general, multilingual, coding, long context, reasoning

### 4.2 DPO Instead of PPO
- Replaces PPO with **Direct Preference Optimization** (DPO)
- Better stability, easier implementation
- Reduces variance in preference learning compared to PPO

### 4.3 Enhanced Capabilities

#### 4.3.1 Code
- Trained code experts with continued pretraining on 1T code tokens
- LCFT (Long-context Finetuning) for 16K code windows
- SFT + rejection sampling specifically targeted at code correctness and readability

#### 4.3.2 Multilingual
- Upsampled non-English tokens during pretraining
- Included synthetic instruction tuning in other languages

#### 4.3.3 Reasoning & Math
- Filtered incorrect traces with reward models
- MCTS used to generate correct multi-step chains
- Self-correction from failed generations

#### 4.3.4 Long-context (in SFT and DPO)
- Hierarchical summarization, QA over large context
- Used 0.1% synthetic long-context data for robust adaptation

#### 4.3.5 Tool Use
- Brave Search, Python interpreter, Wolfram Alpha API
- Tool chaining via step-by-step planning in multi-turn dialogues

#### 4.3.6 Factuality & Hallucination Mitigation
- Added real-time feedback via tools to reduce hallucinations
- Execution verification in code and math

#### 4.3.7 Steerability
- Used system prompts for style, tone, format
- Fine-grained response control

## 5. Multi-Modal Integration (In Progress)

### 5.1 Multi-modal Encoder Overview
- Separate image and speech encoders trained on paired datasets
- Adapter-based integration for cross-modal reasoning

### 5.2 Vision Adapter
- Cross-attention layers map image embeddings into LLM hidden states
- Task-specific adapters used for retrieval, captioning, VQA

### 5.3 Speech Adapter
- Speech encodings converted to token-like embeddings
- Directly fed into the LLM decoder via adapter


## 6. Evaluation and Results

- LLaMA 3.1 405B achieves GPT-4-level performance on reasoning, coding, tool use
- Top-tier multilingual and long-context benchmarks
- Evaluation includes ARC, GSM8K, BFCL, InfiniteBench

## 7. Conclusion and Outlook

- LLaMA 3 is not only a scaled-up LLaMA 2
- Major changes in tokenizer, scaling, alignment, safety, and modality
- Next steps: full release of multimodal variants, extended tool chains, and continual tuning pipelines
